{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9384fa4",
   "metadata": {},
   "source": [
    "今天我们会学习从豆瓣上爬取数据，帮助静静解决问题。你将会完成以下内容：\n",
    "\n",
    "1. 向网页发送请求，获取网页源代码;\n",
    "2. 解析源代码，提取想要的数据；\n",
    "3. 使用 jieba 模块将语句切分成词；\n",
    "4. 运用字典的知识进行词频统计；\n",
    "5. 将词语生成词云图。\n",
    "\n",
    "今天我们完成前两个步骤。后面将会学习剩下的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8c6ce",
   "metadata": {},
   "source": [
    "# 分析网页"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b932b",
   "metadata": {},
   "source": [
    "想要获取网页中的短评，首先要获取网页 HTML 代码，再把短评从中提取出来。\n",
    "\n",
    "在之前的课程中学习过，我们要向网页的服务器发送请求，服务器返回的响应就是网页 HTML 代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6b32b",
   "metadata": {},
   "source": [
    "想要获取网页中的 HTML 代码，首先，我们需要使用 import 导入 requests 模块。\n",
    "\n",
    "将豆瓣电影评论的 URL 地址👇，赋值给变量url。\n",
    "https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\n",
    "\n",
    "接着，将变量 url 传入requests.get()，向对应的服务器发送 GET 请求，返回的 Response 赋值给变量 response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09732d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [418]>\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# TODO 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\"\n",
    "\n",
    "# TODO 使用requests发起GET请求，赋值给response\n",
    "response = requests.get(url)\n",
    "\n",
    "# TODO 使用print输出response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890e16",
   "metadata": {},
   "source": [
    "状态码418\n",
    "欸？返回一个状态码 418(°ー°〃)\n",
    "\n",
    "418 其实是一个愚人节玩笑，含义是：当客户端给一个茶壶发送泡咖啡的请求时，茶壶就返回一个 418 错误状态码，表示“我是一个茶壶”。\n",
    "\n",
    "在这里，状态码 418 也意味着我们的爬虫被豆瓣“发现”了，这该怎么办呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab5a7",
   "metadata": {},
   "source": [
    "# 请求网页"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7e39e",
   "metadata": {},
   "source": [
    "## User-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b97ee3",
   "metadata": {},
   "source": [
    "在进行 HTTP 请求的时候，除了请求指定的 url 信息之外，还会告诉服务端“我是谁”，“我支持哪些特性”。\n",
    "\n",
    "这些信息就是 HTTP 请求头 ，其中声明“我是谁”的就是 HTTP 请求头部的 User-Agent 。\n",
    "\n",
    "由于我们用的是 requests 发起的请求，而不是浏览器，所以我们需要伪装成 Chrome 浏览器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9eecfc",
   "metadata": {},
   "source": [
    "在网页中请求头和响应头信息保存在变量 headers 中，查看请求头或响应头的方法为：\n",
    "\n",
    "1. 使用 响应消息.request.headers 就能查看请求头中的信息。\n",
    "2. 使用 响应消息.headers 就能查看响应头中的信息。\n",
    "\n",
    "接下来，请求链接\n",
    "https://nocturne-spider.baicizhan.com/2020/08/07/1/\n",
    "获取响应消息，输出请求头中的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416b76d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.25.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# 从bs4中导入BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 将URL地址赋值给变量url\n",
    "url = \"https://nocturne-spider.baicizhan.com/2020/08/07/1/\"\n",
    "\n",
    "# 将变量url传入requests.get()，赋值给r\n",
    "r = requests.get(url)\n",
    "\n",
    "# TODO 使用print()输出响应消息中的request.headers\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d824e",
   "metadata": {},
   "source": [
    "请求头中的 User-Agent 用来表示产生请求时浏览器的类型。\n",
    "\n",
    "查看使用 响应消息.request.headers 输出的请求头中, User-Agent 的值为 python-requests ，说明此次请求是Python发出的并且能被服务器识别到。\n",
    "\n",
    "那么怎么伪装成浏览器发出请求呢？接下来，学习浏览器中 User-Agent 的样式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d9bae",
   "metadata": {},
   "source": [
    "1. 设置请求头     \n",
    "\n",
    "我们可以对请求头信息进行设置，请求头中有很多内容，常用的就是User-Agent。\n",
    "\n",
    "请求头信息需要以字典key-value的形式赋值给字典 headers。    \n",
    "\n",
    "2. headers参数      \n",
    "\n",
    "在爬虫程序中添加请求头信息，也就是在请求链接时，带着参数headers。\n",
    "\n",
    "在 requests.get() 中添加参数 headers，其参数值为字典 headers 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9159d",
   "metadata": {},
   "source": [
    "现在我们模拟一个请求头信息， {\"User-Agent\":\"JingJing\"} 赋值给headers。\n",
    "\n",
    "使用 get()函数请求链接时，带上headers参数，并使用 响应消息.request.headers 就能查看请求头中的信息。\n",
    "\n",
    "查看输出结果，我们就已经修改了，那么浏览器 \"User-Agent\" 的值为多少呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cd06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'JingJing', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# 从bs4中导入BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 将URL地址赋值给变量url\n",
    "url = \"https://nocturne-spider.baicizhan.com/2020/08/07/1/\"\n",
    "\n",
    "# TODO 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"JingJing\"}\n",
    "\n",
    "# TODO 将字典headers传递给headers参数，添加进requests.get()中，赋值给r\n",
    "r = requests.get(url, headers = headers)\n",
    "\n",
    "# TODO 使用print()输出响应消息中的request.headers\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0e0b6",
   "metadata": {},
   "source": [
    "## 获取User-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dab2a2",
   "metadata": {},
   "source": [
    "1. 打开网页，右键单击\"检查\"；    \n",
    "\n",
    "2. 点击\"Network\"，并刷新网页；    \n",
    "\n",
    "3. 找(第一个)文件，查看Request Headers的User-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6a1cf",
   "metadata": {},
   "source": [
    "设置请求头      \n",
    "\n",
    "我们用浏览器打开网页，就是浏览器对服务端发送的请求，刚刚查看的请求头就是浏览器的信息。\n",
    "\n",
    "接下来，可以设置请求头信息，模拟成浏览器去访问网站。\n",
    "\n",
    "复制浏览器的 User-Agent 的值，粘贴到代码中，以字典key-value的形式赋值给字典 headers。\n",
    "\n",
    "在 requests.get() 中添加参数 headers，其参数值为字典 headers 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80c166",
   "metadata": {},
   "source": [
    "在课程中，我们可以使用静静电脑浏览器中的 User-Agent          \n",
    "\n",
    "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\n",
    "\n",
    "在程序中添加请求头，将 User-Agent 以字典键对形式赋值给字典headers。\n",
    "\n",
    "在 requests.get() 中添加 headers 参数，其参数值为字典 headers 。\n",
    "\n",
    "接着将服务器响应内容以字符串形式输出，即可获得URL对应的网页内容。由于网页内容较多，我们这里可以用切片的方法，先输出前1000个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ffe622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"zh-CN\" class=\"ua-mac ua-webkit\">\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "    <meta name=\"renderer\" content=\"webkit\">\n",
      "    <meta name=\"referrer\" content=\"always\">\n",
      "    <meta name=\"google-site-verification\" content=\"ok0wCgT20tBBgo9_zat2iAcimtN4Ftf5ccsh092Xeyw\" />\n",
      "    <title>\n",
      "\n",
      "飞屋环游记 短评\n",
      "</title>\n",
      "    \n",
      "    <meta name=\"baidu-site-verification\" content=\"cZdR4xxR7RxmM4zE\" />\n",
      "    <meta http-equiv=\"Pragma\" content=\"no-cache\">\n",
      "    <meta http-equiv=\"Expires\" content=\"Sun, 6 Mar 2005 01:00:00 GMT\">\n",
      "    \n",
      "    <meta name=\"keywords\" content=\"飞屋环游记,Up,影讯,排片,放映时间,电影票价,在线购票\"/>\n",
      "    <meta name=\"description\" content=\"飞屋环游记短评\" />\n",
      "    <meta name=\"mobile-agent\" content=\"format=html5; url=https://m.douban.com/movie/subject/2129039/comments\"/>\n",
      "    <script type=\"text/javascript\" src=\"https://img3.doubanio.com/f/shire/2c0c1c6b83f9a457b0f38c38a32fc43a42ec9bad/js/do.js\" data-cfg-autoload=\"false\"></script>\n",
      "\n",
      "    <link rel=\"apple-touch-icon\" href=\"https://i\n"
     ]
    }
   ],
   "source": [
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\"\n",
    "\n",
    "# TODO 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# TODO 将字典headers传递给headers参数，添加进requests.get()中，赋值给response\n",
    "response = requests.get(url, headers = headers)\n",
    "\n",
    "# TODO 使用.text属性获取网页前1000个字符的内容，并赋值给html\n",
    "html = response.text[:1000]\n",
    "\n",
    "# TODO 使用print输出html\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36171f",
   "metadata": {},
   "source": [
    "# 解析网页"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad31dfa",
   "metadata": {},
   "source": [
    "太好了，获取到网页 HTML 代码。接下来，就需要解析网页，从网页中提取想要的数据。\n",
    "\n",
    "在上节课中我们学习了 BeautifulSoup 解析模块，创建一个 BeautifulSoup 对象；接着使用 find_all() 函数，提取出相关的节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f436875",
   "metadata": {},
   "source": [
    "我们要使用 BeautifulSoup 解析库，需要从 bs4 库中导入 BeautifulSoup\n",
    "\n",
    "使用 BeautifulSoup() 函数，创建一个 BeautifulSoup 对象，传入变量 html 和解析器 lxml，将该对象赋值给变量 soup。    \n",
    "\n",
    "```\n",
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# TODO 从bs4中导入BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\"\n",
    "\n",
    "# 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# 将 url 和 headers参数，添加进requests.get()中，将字典headers传递headers参数，给赋值给response\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# TODO 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# TODO 使用print输出soup\n",
    "print(soup)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcf26e",
   "metadata": {},
   "source": [
    "创建好 BeautifulSoup 对象后，接下来就要提取出短评所在的节点。\n",
    "\n",
    "我们可以使用 BeautifulSoup 中的 find_all() 函数，它可以查询所有符合条件的元素，并组成一个列表。\n",
    "\n",
    "在这里，find_all() 函数中传入的参数内容是什么呢？我们需要去网页中找到短评所在的节点。    \n",
    "\n",
    "如何内容定位可以参考前面几章的笔记..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85231d70",
   "metadata": {},
   "source": [
    "观察网页可以发现：\n",
    "\n",
    "所有的评论都在` <span> `标签中，检索span标签在网页中出现了100多次，而显然，该页只有二十条评论。\n",
    "\n",
    "也可以试试以` <p> `节点来定位，网页中p标签出现的次数也大于20。\n",
    "\n",
    "那么我们该定位哪个节点，才能提取评论呢？    \n",
    "\n",
    "发现了么，短评所在的节点都是类似的它们都有一样的标签和属性。\n",
    "\n",
    "**在 HTML 中，span 标签的作用是用来组合文档中的行内元素。**\n",
    "\n",
    "**class 属性的作用是用来给标签分类。class=\"short\"也就代表着，这里标签的内容是短评。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef704ae",
   "metadata": {},
   "source": [
    "**在网页中属性需要使用 (点).属性值 来定位。**\n",
    "\n",
    "例如，图中我们在检索框中输入 .short 进行搜索，就能看到该页中 class=\"short\" 属性出现的次数为20次。\n",
    "\n",
    "那么，就定位到了提取评论的节点 class=\"short\" 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf2f09",
   "metadata": {},
   "source": [
    "以属性为查询条件就需要在 find_all() 中，传入 class=\"short\"。\n",
    "\n",
    "由于 class 在 Python 里是一个关键字 ，所以后面需要加一个下划线，即 `class_=\"short\"`。 \n",
    "\n",
    "```\n",
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# 从bs4中导入BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\"\n",
    "\n",
    "# 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# 将 url 和 headers参数，添加进requests.get()中，将字典headers传递headers参数，给赋值给response\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# TODO 使用find_all()查询soup中class=\"short\"的节点，赋值给content_all\n",
    "content_all = soup.find_all(class_=\"short\")\n",
    "\n",
    "# TODO 使用print输出content_all\n",
    "print(content_all)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1caeff",
   "metadata": {},
   "source": [
    "由于 find_all() 返回的是一个列表，我们不能直接访问 .string 属性。\n",
    "\n",
    "使用 for 循环遍历列表 content_all，访问 .string 获取每个节点中标签的内容，赋值给变量 contentString。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06199acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后那些最无聊的事情，才是我最怀念的。\n",
      "有史以来最强壮的老爷爷和最轻便的房屋~一群最不靠谱的狗和一只引人发笑的怪鸟~外加一位惹人喜爱却戏份不多的老奶奶，组成了一部我大爱的好片！\n",
      "我心中的结局：就在老爷爷被勒令搬进养老院的第二天，来接他的人发现老爷爷静静地坐在椅子上，手中拿着还没充完的气球。他已经死了，脸上留着幸福的笑容。飞屋冒险只是老爷爷死前的幻境。至此影片结束。\n",
      "最让我感动的是电影开头，这对夫妻志趣相投、相濡以沫，我想自己能不能找到这样的另一半呢\n",
      "那老头为啥不早点买机票带老太太去仙境瀑布？\n",
      "从头哭到尾\n",
      "前面几分钟就让人落泪，后面却越来越不靠谱\n",
      "史上第二彪悍钉子户的离奇拆迁故事。Pixar完美地成熟了。\n",
      "找回丟失已久的夢想吧！在我還未老去的時候···\n",
      "爱和梦想，勇气和希望\n",
      "“我总是记得那些无聊的小事情……”“感谢你给了我精彩的一生”，简单并深情着\n",
      "去掉小孩，坏蛋，狗，鸟的话，这会是部完美的短片，but......;\r\n",
      "那段夫妻蒙太奇应该是我迄今最喜欢的pixar段落了;\r\n",
      "有一点不得不佩服pixar，拍的第一部3D就能完全摒弃滥斛噱头；而片头前那个“国内首部真人3D电影”的三分钟预告片，足以列入反面教材的经典了\r\n",
      "\r\n",
      "\n",
      "在影片最开头对老夫妻一生的回顾无疑是最大泪点，他们过了很好很幸福的一生，当垂垂老矣，他们仍深爱着对方，就算你已不在，也要带着他们曾经共同的梦想在生命的最终冒险一把，就像小朋友不畏惧一切的好奇心，对生活对梦想，再大的年纪都不是阻碍\n",
      "气球升起的时候太美好了\n",
      "基本是儿童剧，最感人的部分一直到屋子初飞起的那一刻，之后就一直在梦游了..=。= 原本以为一切奇想不过是卡尔在再遇女孩那晚所做的一场梦，并且第二天天亮她还会来找他的…对男孩子气的艾莉的印象实在是太深了，这片要是只选取到艾莉死时，卡尔飞起气球，就是满分。皮克斯的技术没得说，因此4星\n",
      "近乎无可挑剔..\n",
      "PIXAR总是这么perfect.原来和爱人相守一生是最大的冒险\n",
      "几十年的夫妻生活浓缩成几分钟那段美好死了。\n",
      "触动内心最柔软的温情\n",
      "梦想，多久开始实现都不晚。《冰河世纪》也好，《史瑞克》也好，永远都赶不上皮克斯动画的情怀与意境。\n"
     ]
    }
   ],
   "source": [
    "# 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# 从bs4中导入BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/subject/2129039/comments?sort=new_score&status=P\"\n",
    "\n",
    "# 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# 将 url 和 headers参数，添加进requests.get()中，将字典headers传递headers参数，给赋值给response\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# 使用find_all()查询soup中class=\"short\"的节点，赋值给content_all\n",
    "content_all = soup.find_all(class_=\"short\")\n",
    "\n",
    "# TODO for循环遍历content_all\n",
    "for content in content_all:\n",
    "\n",
    "    # TODO 获取每个节点标签的内容，赋值给contentString\n",
    "    contentString = content.string\n",
    "\n",
    "    # TODO 使用print输出contentString\n",
    "    print(contentString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfaadcf",
   "metadata": {},
   "source": [
    "通过前面的学习，我们就完成了网页中短评内容的提取。\n",
    "\n",
    "明天我们将继续学习，完成短评语句的分词处理，最后生成词云图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36846609",
   "metadata": {},
   "source": [
    "# 百词斩题目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bef9d",
   "metadata": {},
   "source": [
    "## 怦然心动     \n",
    "\n",
    "静静最喜欢的电影名字叫“怦然心动”，她想要通过学到的知识，查询这部电影是否进入了豆瓣电影的top25。豆瓣top25电影的网页URL地址是https://movie.douban.com/top250?start=0&filter=\n",
    "，这该怎么实现呢？\n",
    "\n",
    "解题思路：\n",
    "1. 获取User-Agent，设置请求头；\n",
    "2. 将url和headers参数，添加进requests.get()中，获取网页HTML代码；\n",
    "3. 创建一个BeautifulSoup对象；\n",
    "4. 使用find_all()函数找到所有的class=\"title\"节点；\n",
    "5. 遍历并使用.string属性获取电影名字，判断电影名为怦然心动时，输出\"怦然心动是豆瓣top25电影\"。\n",
    "\n",
    "User-Agent：Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8185e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "怦然心动是豆瓣top25电影\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# TODO 从bs4中导入BeautifulSoup模块\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# TODO 将豆瓣电影评论URL地址，赋值给变量url\n",
    "url = \"https://movie.douban.com/top250?start=0&filter=\"\n",
    "\n",
    "# TODO 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# TODO 将 url 和 headers参数，添加进requests.get()中，将字典headers传递headers参数，给赋值给response\n",
    "response = requests.get(url, headers = headers)\n",
    "\n",
    "# TODO 将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# TODO 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# TODO 使用find_all()查询soup中class=\"title\"的节点，赋值给content_all\n",
    "content_all = soup.find_all(class_ = \"title\")\n",
    "\n",
    "# TODO for循环遍历content_all\n",
    "for content in content_all:\n",
    "    # TODO 获取每个节点中标签内的内容，赋值给contentString\n",
    "    contentString = content.string\n",
    "\n",
    "    # TODO 使用if判断contentString等于\"怦然心动\"时\n",
    "    if contentString == \"怦然心动\":\n",
    "        # TODO 使用print()输出\"怦然心动是豆瓣top25电影\"\n",
    "        print(\"怦然心动是豆瓣top25电影\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97573f3d",
   "metadata": {},
   "source": [
    "## 电影奖项     \n",
    "\n",
    "静静在知乎搜索韩国电影《寄生虫》的时候，看到了这部电影收获了许多的奖项。于是，她想利用刚刚学过的爬虫小知识，获取该电影获得的所有的奖项。\n",
    "\n",
    "已知简介的节点是class=\"TopicMovieIntro-awardItemTitle\"。\n",
    "\n",
    "URL链接：https://www.zhihu.com/topic/20753544/intro\n",
    "\n",
    "User-Agent：Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\n",
    "\n",
    "注意：本题需要用到.text方法知识点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c2f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第13届底特律影评人协会奖\n",
      "第14届亚洲电影大奖\n",
      "第18届华盛顿影评人协会奖\n",
      "第20届美国电影学会奖\n",
      "第22届英国独立电影奖\n",
      "第23届好莱坞电影奖\n",
      "第23届美国在线影评人协会奖\n",
      "第24届美国艺术指导工会奖\n",
      "第24届金卫星奖\n",
      "第25届评论家选择奖\n",
      "第26届美国演员工会奖\n",
      "第31届美国制片人工会奖\n",
      "第32届芝加哥影评人协会奖\n",
      "第35届独立精神奖\n",
      "第40届伦敦影评人协会奖\n",
      "第40届波士顿影评人协会奖\n",
      "第40届韩国青龙电影奖\n",
      "第45届法国电影凯撒奖\n",
      "第45届洛杉矶影评人协会奖\n",
      "第49届鹿特丹国际电影节\n",
      "第4届亚特兰大影评人协会奖\n",
      "第54届美国国家影评人协会奖\n",
      "第56届韩国电影大钟奖\n",
      "第56届韩国百想艺术大赏\n",
      "第67届美国音效剪辑协会奖\n",
      "第70届美国剪辑工会奖\n",
      "第72届戛纳电影节\n",
      "第72届美国导演工会奖\n",
      "第72届美国编剧工会奖\n",
      "第73届英国电影和电视艺术学院奖\n",
      "第77届金球奖\n",
      "第85届纽约影评人协会奖\n",
      "第91届美国国家评论协会奖\n",
      "第92届奥斯卡金像奖\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# TODO 从bs4中导入BeautifulSoup模块\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# TODO 将\"https://www.zhihu.com/topic/20753544/intro\"，赋值给变量url\n",
    "url = \"https://www.zhihu.com/topic/20753544/intro\"\n",
    "\n",
    "# TODO 将User-Agent以字典键对形式赋值给headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\"}\n",
    "\n",
    "# TODO 将 url 和 headers参数，添加进requests.get()中，将字典headers传递headers参数，给赋值给response\n",
    "response = requests.get(url, headers = headers)\n",
    "\n",
    "# TODO 将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# TODO 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# TODO # 使用find_all()查询soup中class=\"TopicMovieIntro-awardItemTitle\"的节点，赋值给content_all\n",
    "content_all = soup.find_all(class_ = \"TopicMovieIntro-awardItemTitle\")\n",
    "\n",
    "# TODO for循环遍历content_all\n",
    "for content in content_all:\n",
    "\n",
    "    # TODO 使用.text方法获取每个节点中标签内容，赋值给contentString\n",
    "    contentString = content.text\n",
    "    \n",
    "    # TODO 输出contentString\n",
    "    print(contentString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29f604",
   "metadata": {},
   "source": [
    "## 每日新闻      \n",
    "\n",
    "静静要爬取网易的每日新闻标题并输出内容，链接\n",
    "http://nocturne.bczcdn.com/zip/1625207762993_63705/web.html\n",
    "每个新闻的标题都嵌套在class=\"rank\"的标签的下一级a标签。\n",
    "\n",
    "【题目要求】\n",
    "1. 有的tr标签中内容是空的，所以找到class=\"rank\"标签；\n",
    "2. 找到class=\"rank\"标签内的a标签；\n",
    "3. 使用.string获取所有的文字内容并输出。\n",
    "\n",
    "注意：由于 class 在 Python 里是一个关键字 ，为了避免关键词重复，所以后面需要加一个下划线，即 class_=\"rank\"，这里节点先使用find_all(class_=\"rank\")。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f537a",
   "metadata": {},
   "source": [
    "```\n",
    "# TODO 使用import导入requests模块\n",
    "import requests\n",
    "\n",
    "# TODO 使用from...import从bs4中导入BeautifulSoup模块\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# TODO 将URL地址赋值给变量url\n",
    "url = \"http://nocturne.bczcdn.com/zip/1625207762993_63705/web.html\"\n",
    "\n",
    "# TODO 将变量url传入requests.get()，赋值给response\n",
    "response = requests.get(url)\n",
    "\n",
    "# TODO 使用.text将服务器响应内容转换为字符串形式，赋值给html\n",
    "html = response.text\n",
    "\n",
    "# TODO 使用BeautifulSoup()读取html，添加lxml解析器，赋值给soup\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# TODO 使用find_all()查询soup中class=\"rank\"节点，赋值给content_all\n",
    "content_all = soup.find_all(class_ = \"rank\")\n",
    "\n",
    "# TODO for循环遍历content_all\n",
    "for content in content_all:\n",
    "\n",
    "    # TODO 使用find_all()查询name=\"a\"的节点，赋值给title_list\n",
    "    title_list = content.find_all(name = \"a\")\n",
    "\n",
    "    # TODO for循环遍历title_list\n",
    "    for title in title_list:\n",
    "        # TODO 使用print()输出标签中的内容\n",
    "        print(title.string)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
